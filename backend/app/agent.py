import os
from langgraph.graph import StateGraph
from langgraph.prebuilt import create_react_agent
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI
from typing_extensions import TypedDict
from langchain_core.agents import AgentFinish
from langchain_community.agent_toolkits.sql.base import create_sql_agent
from langchain.agents.agent_types import AgentType
from langchain.prompts import PromptTemplate
from langchain.tools import tool
import traceback

# === Define Chat State Schema ===
class ChatState(TypedDict):
    email_content: str
    user_query: str
    summary: str

llm = ChatOpenAI(
    model="gemma-3-27b-it",
    openai_api_key="EMPTY",
    openai_api_base=os.getenv("VLLM_API_BASE")
)

# === Tool Configuration ===
DATABASE_URL = f"postgresql://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('POSTGRES_HOST')}:{os.getenv('POSTGRES_PORT')}/{os.getenv('POSTGRES_DB')}"
db = SQLDatabase.from_uri(DATABASE_URL)
toolkit = SQLDatabaseToolkit(llm=llm, db=db)

# Add table info and foreign key hints
table_info = db.get_table_info()
table_info += """
-- Foreign key relationships:
- systeminfo.id → server.system_id
- lan_system_map.lan → system_lan_info.lan_key
- lan_system_map.id → systeminfo.id
- mlan_system_map.mlan → system_mlan_info.mlan_key
- mlan_system_map.id → systeminfo.id
- psu_system_map.psu → system_psu_info.psu_key
- psu_system_map.id → systeminfo.id
- server_pcie_map.pcieinfo → pcie_info.pcie_key
- server_pcie_map.(projectmodel, gbtsn) ↔ server.(projectmodel, gbtsn)
- storage_connector_map.connector → connector_info.connector_key
- storage_connector_map.id → storageinfo.id
- server_storage_map.id → storageinfo.id
- server_storage_map.(projectmodel, gbtsn) ↔ server.(projectmodel, gbtsn)
"""
# === Agent Configuration ===
agent_sql = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
)

def is_natural_query(text: str) -> bool:
    email_indicators = ["subject:", "dear", "regards", "best", "sincerely", "message", "thank you"]
    if any(word in text.lower() for word in email_indicators) or len(text.split("\n")) > 5:
        return False  # It's likely an email
    return True  # It's likely a direct query

# Import tools from agent_tools.py
from .agent_tools import run_sql_agent, fetch_and_analyze_web_html_node

# === Email-to-Query Prompt Template ===
email_parse_prompt = PromptTemplate.from_template("""
From the email below, extract a clear and concise query intention in English that describes what information the user wants from the database.
===
{email}
===
Only respond with the query intention.
""")

parse_chain = email_parse_prompt | llm


# === Node Definitions ===
def parse_email(state: ChatState) -> ChatState:
    print("[DEBUG] parse_email - input state:", state)
    email = state.get("email_content", "").strip()
    if is_natural_query(email):
        user_query = email
    else:
        response = parse_chain.invoke({"email": email})
        user_query = response.content.strip()
    new_state: ChatState = {
        "email_content": email,
        "user_query": user_query,
        "summary": state.get("summary", "")
    }
    print("[DEBUG] parse_email - output state:", new_state)
    return new_state


def generate_email_reply(state: ChatState) -> ChatState:
    print("[DEBUG] generate_email_reply - input state:", state)
    reply = f"""
    Dear Client,

    Thank you for your inquiry. Below is the summarized server information based on your request:
    {state.get('summary', '')}

    If you have any further questions, feel free to reach out.

    This email was automatically generated by BDM.chat assistant.
    """
    print("\n=== Generated Email Response ===\n")
    print(reply)
    print("[DEBUG] generate_email_reply - output state:", state)
    return state


# === LLM Agent Tool Selection Node ===
from langchain.agents import ZeroShotAgent, AgentExecutor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Set custom prompt
prefix = (
    "You are a professional data query and web analysis assistant. You can only answer questions using the following tools. Please choose the most appropriate tool based on the description."
)
suffix = "Question: {input}\n{agent_scratchpad}"
prompt = ZeroShotAgent.create_prompt(
    [run_sql_agent, fetch_and_analyze_web_html_node],
    prefix=prefix,
    suffix=suffix,
    input_variables=["input", "agent_scratchpad"]
)
llm_chain = LLMChain(llm=llm, prompt=prompt)
agent = ZeroShotAgent(
    llm_chain=llm_chain,
    tools=[run_sql_agent, fetch_and_analyze_web_html_node],
    verbose=True
)
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=[run_sql_agent, fetch_and_analyze_web_html_node],
    verbose=True
)

def extract_summary(summary):
    # Recursively extract summary string
    while isinstance(summary, dict):
        summary = summary.get("summary", "")
    return summary

def llm_agent_node(state: ChatState) -> ChatState:
    query = state.get("user_query", "")
    print("[DEBUG] llm_agent_node - input state:", state)
    try:
        result = agent_executor.invoke({"input": query})
        if isinstance(result, dict):
            summary = result.get("summary", result.get("output", str(result)))
            summary = extract_summary(summary)
        else:
            summary = str(result)
    except Exception as e:
        summary = f"Error occurred during agent execution: {e}"
    new_state: ChatState = {
        "email_content": state.get("email_content", ""),
        "user_query": query,
        "summary": summary
    }
    print("[DEBUG] llm_agent_node - output state:", new_state)
    return new_state

    
graph = StateGraph(ChatState)
graph.add_node("parse_email", parse_email)
graph.add_node("llm_agent_node", llm_agent_node)
graph.add_node("generate_email_reply", generate_email_reply)

# Entry point
graph.set_entry_point("parse_email")
graph.add_edge("parse_email", "llm_agent_node")
graph.add_edge("llm_agent_node", "generate_email_reply")

workflow = graph.compile()

def run_agent_workflow(email_content: str):
    state: ChatState = {
        "email_content": email_content,
        "user_query": "",
        "summary": ""
    }
    print("[DEBUG] Initial workflow state:", state)
    try:
        result = workflow.invoke(state)
        print("[DEBUG] Final workflow result:", result)
        return result
    except Exception as e:
        print("[ERROR] Exception in workflow.invoke:")
        traceback.print_exc()
        # You can choose to return a special ChatState or raise directly
        return {"email_content": email_content, "user_query": state.get("user_query", ""), "summary": f"Exception: {e}"}

